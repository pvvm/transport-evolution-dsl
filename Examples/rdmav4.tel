// A: the runtime will automatically do the rate limiting

// A: transmit_queue.rate() to set the rate of the transmit queue

event RDMA_EVENT {
    int wr_id;
    int lkey;
    addr_t addr;
    int length;
}

event SEND_WQE : RDMA_EVENT {
    int ack_req;
}

event WRITE_WQE : RDMA_EVENT {
    int ack_req;
    int rkey;
    addr_t raddr; 
}

event READ_WQE : RDMA_EVENT {
    int rkey;
    addr_t raddr; 
}

// Note: Atomic operations use 64 bit fields
// A: bit<n> in headers
event ATOMIC_WQE : RDMA_EVENT {
    int swap_add;
    int compare;
    int rkey;
    addr_t raddr;
}

event RECV_WQE : RDMA_EVENT {

}

event TRANSMIT_EVENT {
    int qp_id;
    int wr_id;
    int psn;
    bool ack_req;
}

event ACK {
    int qp_id;
    int wr_id;
    // everything prior and including this number is received
    int psn; 
    int credit_count;
    int MSN;
}

event NACK {
    int qp_id;
    int wr_id;
    bool type_nack;
    int psn;
    float RNR_delay;
}

event READ_REQ_DATA {
    int qp_id;
    int wr_id;
    int psn;
    addr_t raddr;
    int rkey;
    bool ecn;
}

event READ_RESP_DATA {
    int qp_id;
    int wr_id;
    int psn;
    stream data;
}

event WRITE_DATA {
    int qp_id;
    int wr_id;
    int psn;
    bool ack_req;
    addr_t raddr;
    int rkey;
    int length;
    bool ecn;
    stream data;
}

event RECV_DATA {
    int qp_id;
    int wr_id;
    int psn;
    bool ack_req;
    stream data;
    int opcode;
    bool ecn;
}

event ATOMIC_DATA {
    int qp_id;
    int swap_add;
    int compare;
    int rkey;
    addr_t raddr;
    bool ecn;
    int length = 4;
}

event CNP_CHECK {
    int qp_id;
}

event CNP {
    int qp_id;
}

event DCQCN_TIMER {
    int qp_id;
}

event DCQCN_INCREASE {
    int qp_id;
}

event ALPHA_CHECK {
    int qp_id;
}

interm_output iterm_out {
}

struct work_comp {
    int wr_id;
    int status;
    int opcode;
    int qp_id;
    int length;
}

header IPHeader {
    bool ecn;
    bit<16> identification;
    bit<3> flags;
    bit<8> ttl;
    bit<8> protocol;
    bit<16> hdrChecksum;
    addr_t srcAddr;
    addr_t dstAddr;
}

header UDPHeader {
    int src_port;
    int dst_port;
    int length;
    int checksum;
}

header BTHeader {
    int opcode;
    bool ack_req;
    int dest_qp;
    int psn;
}

header RETHeader {
    int raddr;
    int rkey;
    int length;
}

header AETHeader {
    bool ack;           // 1 if ack, 0 if nack
    bool type_nack;     // 1 if OOO, 0 if RNR
    int credit_count;
    int MSN;
    float RNR_delay;
}

header AtomicETHeader {
    addr_t raddr;
    int rkey;
    int swap_add;
    int compare;
}

header AtomicAckETHeader {
    int original_data;
}

struct packet_list {
    pkt_t packet;
    int psn;
    bool ack_req;
    int wr_id;
    bool acked;
    int SSN;
}

struct ack_wait {
    timer_t ack_timer;
    int psn;
}

context myContext {
    int qp_id;
    int lkey;
    list<RDMA_EVENT> SQ_list;
    list<RDMA_EVENT> RQ_list;
    list<packet_list> sent_packets;
    int nPSN;
    int transport_timer;
    stream CQ;

    // Responder side
    int ePSN;
    int MSN;
    int credit_count = 0;
    int recv_first_psn;
    int write_first_psn;

    // Requester side
    int SSN = 0;
    int LSN = 0;

    int MTU;
    int init_sqn;
    int dest_qp;
    int src_port;
    int dst_port;

    // DCQCN
    int Rt;
    int Rc;
    int alpha;
    int byte_counter;
    int BC;
    int T;
    int F;
    int Rai;
    int ecn_found_counter = 0;

    bool first_pkt_connection = true;

    // Timers
    list<ack_wait> ack_timeout;
    timer_t timer_cnp;
    timer_t timer_alpha;
    timer_t timer_DCQCN_counter;
}

tx_sched tx_module {
    tx_queue_t transmit_queue(20)[2];

    int packet_counter = 0;
    int currently_sending = 0;

    // TODO:
    // function that sends information to application

    pkt_t next_packet() {
        int slice = 10;
        pkt_t p = transmit_queue[currently_sending].pop();
        packet_counter = packet_counter + 1;

        if(packet_counter == slice || transmit_queue[currently_sending].len() == 0) {
            currently_sending = (currently_sending + 1) % 2;
            packet_counter = 0;
        }

        return p;
    }
}

rx_parser rx_module {

    // TODO:
    event_t application_parser() {

    }

    event_t packet_parser(IPHeader iph, pkt_t packet) {
        UDPHeader udp;
        packet.extract_hdr(udp);
        BTHeader bth;
        packet.extract_hdr(bth);

        int op = bth.opcode;
        
        if(op == 4 || op == 0 || op == 2 || op == 1) {
            RECV_DATA new_event;

            new_event.qp_id = bth.dest_qp;
            new_event.psn = bth.psn;
            new_event.ack_req = bth.ack_req;
            new_event.data = packet.get_data();
            new_event.opcode = bth.opcode;
            new_event.ecn = iph.ecn;
            set_flow_id(new_event, flow_id);
            return new_event;

        } else if(op == 10 || op == 6 || op == 8 || op == 7) {
            RETHeader reth;
            packet.extract_hdr(reth);
            WRITE_DATA new_event;
            new_event.qp_id = bth.dest_qp;
            new_event.psn = bth.psn;
            new_event.ack_req = bth.ack_req;
            new_event.raddr = reth.raddr;
            new_event.rkey = reth.rkey;
            new_event.length = reth.rkey;
            new_event.ecn = iph.ecn;
            new_event.data = packet.get_data();
            set_flow_id(new_event, flow_id);
            return new_event;

        } else if(op == 12) {
            RETHeader reth;
            packet.extract_hdr(reth);
            READ_REQ_DATA new_event;
            new_event.qp_id = bth.dest_qp;
            new_event.psn = bth.psn;
            new_event.raddr = reth.raddr;
            new_event.rkey = reth.rkey;
            new_event.ecn = iph.ecn;
            set_flow_id(new_event, flow_id);
            return new_event;

        } else if(op == 20 || op == 19) {
            AtomicAckETHeader atomicAeth;
            packet.extract_hdr(atomicAeth);
            ATOMIC_DATA new_event;
            new_event.qp_id = bth.dest_qp;
            new_event.swap_add = atomicAeth.swap_add;
            new_event.compare = atomicAeth.compare;
            new_event.rkey = atomicAeth.rkey;
            new_event.raddr = atomicAeth.raddr;
            new_event.ecn = iph.ecn;
            set_flow_id(new_event, flow_id);
            return new_event;

        // Common ack/nack or atomic ack
        } else if(op == 17 || op == 18) {
            AETHeader aeth;
            packet.extract_hdr(aeth);
            if(aeth.ack || op == 18) {
                ACK new_event;
                new_event.qp_id = bth.dest_qp;
                new_event.psn = bth.psn;
                new_event.credit_count = aeth.credit_count;
                new_event.MSN = aeth.MSN;
                set_flow_id(new_event, flow_id);
                return new_event;
            } else {
                NACK new_event;
                new_event.qp_id = bth.dest_qp;
                new_event.psn = bth.psn;
                new_event.type_nack = aeth.type_nack;
                new_event.RNR_delay = aeth.RNR_delay;
                set_flow_id(new_event, flow_id);
                return new_event;
            }

        } else if(op == 16 || op == 13 || op == 15 || op == 14) {
            READ_RESP_DATA new_event;
            new_event.qp_id = bth.qp_id;
            new_event.psn = bth.psn;
            new_event.data = packet.get_data();
            set_flow_id(new_event, flow_id);
            return new_event;
        }

        // Note: if it doesn't match any opcode, what should we return?
        // Instead, the function could return void and we simply push to the type's queue.
        // Problem: this function would need to be inside of the scheduler to access the queues
        // A: have a default event (NULL_EVENT)

        // New note: could we do it in this way?
        NULL_EVENT new_event;
        return new_event;
    }

}


sched ROCEv2Sched {

    // New note: could we have some way to share drop functions among all events?
    // Because we might need to define several drop functions if not
    int common_drop<T> (T queue) {
        return queue.len() - 1;
    }

    queue_t<RDMA_EVENT> SEND_Queue(0, 20, common_drop);
    queue_t<RDMA_EVENT> RECV_Queue(0, 20, common_drop);
    queue_t<TRANSMIT_EVENT> TRANSMIT_Queue(0, 20, common_drop);
    queue_t<READ_REQ_DATA> READ_REQ_DATA_Queue(0, 20, common_drop);
    queue_t<READ_RESP_DATA> READ_RESP_DATA_Queue(0, 20, common_drop);
    queue_t<WRITE_DATA> WRITE_DATA_Queue(0, 20, common_drop);
    queue_t<RECV_DATA> RECV_DATA_Queue(0, 20, common_drop);
    queue_t<ATOMIC_DATA> ATOMIC_DATA_Queue(0, 20, common_drop);
    queue_t<ACK> ACK_Queue(0, 20, common_drop);
    queue_t<NACK> NACK_Queue(0, 20, common_drop);
    queue_t<CNP_CHECK> CNP_CHECK_Queue(0, 20, common_drop);
    queue_t<CNP> CNP_Queue(0, 20, common_drop);
    queue_t<DCQCN_TIMER> DCQCN_TIMER_Queue(0, 20, common_drop);
    queue_t<DCQCN_INCREASE> DCQCN_INCREASE_Queue(0, 20, common_drop);
    queue_t<ALPHA_CHECK> ALPHA_CHECK_Queue(0, 20, common_drop);

    // New note: isn't there a better way to push events to queues?
    // Something like we do in the dispatcher
    bool enqueue(event_t new_event) {
        if(type(new_event) == RDMA_EVENT::SEND_WQE ||
        type(new_event) == RDMA_EVENT::WRITE_WQE ||
        type(new_event) == RDMA_EVENT::READ_WQE ||
        type(new_event) == RDMA_EVENT::ATOMIC_WQE) {
            SEND_Queue.push(new_event);
            return 1;
        } else if(type(new_event) == RDMA_EVENT::RECV_WQE) {
            RECV_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == TRANSMIT_EVENT) {
            TRANSMIT_EVENT_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == READ_REQ_DATA) {
            READ_REQ_DATA_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == READ_RESP_DATA) {
            READ_RESP_DATA_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == WRITE_DATA) {
            WRITE_DATA_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == RECV_DATA) {
            RECV_DATA_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == ATOMIC_DATA) {
            ATOMIC_DATA_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == ACK) {
            ACK_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == NACK) {
            NACK_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == CNP_CHECK) {
            CNP_CHECK_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == CNP) {
            CNP_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == DCQCN_TIMER) {
            DCQCN_TIMER_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == DCQCN_INCREASE) {
            DCQCN_INCREASE_Queue.push(new_event);
            return 1;
        } else if (type(new_event) == ALPHA_CHECK) {
            ALPHA_CHECK_Queue.push(new_event);
            return 1;
        }

        return 0;
    }


    // Note: priority
    // A: NACK / CNP (TIMER vs ACK) DATA / RDMA_EVENT
    // NACK / ACK TIMER DATA / RDMA_EVENT

    // New note: priority updated
    // NACK / DCQCN TIMER EPs / CNP EPs / ACK / DATA / RDMA_EVENT

    event_t next_event() {
        if(!NACK_Queue.active()) {
            return NACK_Queue.pop();

        } else if(!ACK_Queue.active()) {
            return ACK_Queue.pop();

        } else if(!ALPHA_CHECK_Queue.active()) {
            return ALPHA_CHECK_Queue.pop();

        } else if(!DCQCN_TIMER_Queue.active()) {
            return DCQCN_TIMER_Queue.pop();

        } else if(!DCQCN_INCREASE_Queue.active()) {
            return DCQCN_INCREASE_Queue.pop();

        } else if(!CNP_CHECK_Queue.active()) {
            return CNP_CHECK_Queue.pop();

        } else if(!CNP_Queue.active()) {
            return CNP_Queue.pop();

        } else if(!ATOMIC_DATA_Queue.active()) {
            return ATOMIC_DATA_Queue_Queue.pop();

        } else if(!READ_REQ_DATA_Queue.active()) {
            return READ_REQ_DATA_Queue.pop();

        } else if(!READ_RESP_DATA_Queue.active()) {
            return READ_REQ_DATA_Queue.pop();
        
        } else if(!WRITE_DATA_Queue.active()) {
            return WRITE_DATA_Queue.pop();

        } else if(!RECV_DATA_Queue.active()) {
            return RECV_DATA_Queue.pop();
        
        } else if(!TRANSMIT_EVENT_Queue.active()) {
            return TRANSMIT_EVENT_Queue.pop();
        
        } else if(!RECV_Queue.active()) {
            return RECV_Queue.pop();

        } else {
            return SEND_Queue.pop();
        }

        // New note: should this function also return NULL_EVENT if it doesn't match any event type?
    }
}

void SendProcessor(RDMA_EVENT::SEND_WQE ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    ctx.SQ_list.add(ev);

    if(ev.lkey == ctx.lkey) {
        stream buffer;
        buffer.mem_append(ev.addr, ev.length);
        int first_psn;

        for(int index = 0; index < ev.length; index = index + ctx.MTU) {
            pkt_t p = new_pkt();

            int data_len = ctx.MTU;
            if(index + ctx.MTU < ev.length)
                p.add_data(buffer[index : index + ctx.MTU]);
            else { // if the last packet doesn't match MTU
                p.add_data(buffer[index : ev.length]);
                data_len = env.length - index;
            }

            BTHeader bth;
            if(ev.length <= ctx.MTU) {
                bth.opcode = 4;     // only 1 packet
                last_packet = true;
                first_psn = ctx.nPSN;
            } else if(index == 0) {
                bth.opcode = 0;     // first packet
                first_psn = ctx.nPSN;
            } else if(index + ctx.MTU >= ev.length)
                bth.opcode = 2;     // last packet
            else
                bth.opcode = 1;     // middle packet
            bth.ack_req = ev.ack_req;
            bth.dest_qp = ctx.dest_qp;
            bth.psn = ctx.nPSN;
            ctx.nPSN = ctx.nPSN + 1;

            UDPHeader udp;
            udp.src_port = ctx.src_port;
            udp.dst_port = ctx.dst_port;
            udp.length = 8 + bth.len() + data_len;
            // checksum(name_of_the_algorithm)

            p.add_hdr({udp, bth});

            packet_list save_packet;
            // Note: could assign work from one packet to another? or should we have a copy function?
            save_packet.packet = p;
            save_packet.psn = bth.psn;
            save_packet.ack_req = bth.ack_req;
            save_packet.wr_id = ev.wr_id;
            save_packet.acked = false;
            save_packt.SSN = ctx.SSN;
            ctx.SSN = ctx.SSN + 1;

            ctx.sent_packets.add(save_packet);
        }
        TRANSMIT_EVENT new_event;
        new_event.wr_id = ev.wr_id;
        new_event.psn = first_psn;
        new_event.ack_req = ev.ack_req;
        events.add(new_event);
    }
}

void WriteProcessor(RDMA_EVENT::WRITE_WQE ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    ctx.SQ_list.add(ev);

    if(ev.lkey == ctx.lkey) {
        stream buffer;
        buffer.mem_append(ev.addr, ev.length);
        RETHeader reth;
        int first_psn;

        for(int index = 0; index < ev.length; index = index + ctx.MTU) {
            pkt_t p = new_pkt();

            int data_len = ctx.MTU;
            if(index + ctx.MTU < ev.length)
                p.add_data(buffer[index : index + ctx.MTU]);
            else { // if the last packet doesn't match MTU
                p.add_data(buffer[index : ev.length]);
                data_len = ev.length - index;
            }

            if(index == 0) {
                reth.raddr = ev.raddr;
                reth.rkey = ev.rkey;
                reth.length = ev.length;
            }

            BTHeader bth;
            if(ev.length <= ctx.MTU) {
                bth.opcode = 10;     // only 1 packet
                first_psn = ctx.nPSN;
            } else if(index == 0) {
                bth.opcode = 6;     // first packet
                first_psn = ctx.nPSN;
            } else if(index + ctx.MTU >= ev.length)
                bth.opcode = 8;     // last packet
            else
                bth.opcode = 7;     // middle packet
            bth.ack_req = ev.ack_req;
            bth.dest_qp = ctx.dest_qp;
            bth.psn = ctx.nPSN;
            ctx.first_psn = ctx.nPSN;
            ctx.nPSN = ctx.nPSN + 1;

            UDPHeader udp;
            udp.src_port = ctx.src_port;
            udp.dst_port = ctx.dst_port;
            udp.length = 8 + bth.len() + data_len;
            if(index == 0) {
                udp.length = udp.length + eth.len();
                p.add_hdr({udp, bth, reth});
            } else
                p.add_hdr({udp, bth});

            packet_list save_packet;
            // Note: could assign work from one packet to another? or should we have a copy function?
            save_packet.packet = p;
            save_packet.psn = bth.psn;
            save_packet.ack_req = bth.ack_req;
            save_packet.wr_id = ev.wr_id;
            save_packet.acked = false;
            save_packt.SSN = ctx.SSN;
            ctx.SSN = ctx.SSN + 1;

            ctx.sent_packets.add(save_packet);

            ctx.LSN = ctx.LSN + 1;
        }
        TRANSMIT_EVENT new_event;
        new_event.wr_id = ev.wr_id;
        new_event.psn = first_psn;
        new_event.ack_req = ev.ack_req;
        events.add(new_event);
    }
}

void RecvProcessor(RDMA_EVENT::RECV_WQE ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    ctx.RQ_list.add(ev);
    ctx.credit_count = ctx.credit_count + 1;
}

void ReadProcessor(RDMA_EVENT::READ_WQE ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    ctx.SQ_list.add(ev);

    pkt_t p = new_pkt();
    int first_psn;

    RETHeader reth;
    reth.raddr = ev.raddr;
    reth.rkey = ev.rkey;
    reth.length = ev.length;

    BTHeader bth;
    bth.opcode = 12;
    bth.dest_qp = ctx.dest_qp;
    bth.psn = ctx.nPSN;
    first_psn = ctx.nPSN;
    // Note: maybe we'll need a floor/ceiling function or % to get the remainder of the division
    // A: use int()
    // A: also add % (remainder)
    ctx.nPSN = ctx.nPSN + ceil(ev.length / ctx.MTU);

    UDPHeader udp;
    udp.src_port = ctx.src_port;
    udp.dst_port = ctx.dst_port;
    udp.length = 8 + bth.len() + eth.len();

    p.add_hdr({udp, bth, eth});

    for(int i = 0; i < ceil(ev.length / ctx.MTU); i = i + 1) {
        packet_list save_packet;
        // Note: could assign work from one packet to another? or should we have a copy function?
        save_packet.packet = p;
        save_packet.psn = bth.psn + i;
        save_packet.ack_req = true;
        save_packet.wr_id = ev.wr_id;
        save_packet.acked = false;
        save_packt.SSN = ctx.SSN;
        ctx.SSN = ctx.SSN + 1;

        ctx.sent_packets.add(save_packet);

        ctx.LSN = ctx.LSN + 1;
    }
    // make sure to not retransmit the same packet
    TRANSMIT_EVENT new_event;
    new_event.wr_id = ev.wr_id;
    new_event.psn = first_psn;
    new_event.ack_req = ev.ack_req;
    events.add(new_event);
}

void AtomicProcessor(RDMA_EVENT::READ_WQE ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    ctx.SQ_list.add(ev);

    pkt_t p = new_pkt();
    int first_psn;

    AtomicETHeader aeth;
    aeth.raddr = ev.raddr;
    aeth.rkey = ev.rkey;
    aeth.swap_add = ev.swap_add;
    aeth.compare = ev.compare;
    p.add_hdr(aeth);

    BTHeader bth;
    if(ev.compare == -1)  // fetchAdd
        bth.opcode = 20;
    else                    // compSwap
        bth.opcode = 19;
    bth.dest_qp = ctx.dest_qp;
    bth.psn = ctx.nPSN;
    ctx.nPSN = ctx.nPSN + 1;

    UDPHeader udp;
    udp.src_port = ctx.src_port;
    udp.dst_port = ctx.dst_port;
    udp.length = 8 + bth.len() + aeth.len();

    p.add_hdr({udp, bth, aeth});
    packet_list save_packet;
    // Note: could assign work from one packet to another? or should we have a copy function?
    save_packet.packet = p;
    save_packet.psn = bth.psn + i;
    save_packet.ack_req = true;
    save_packet.wr_id = ev.wr_id;
    save_packet.acked = false;
    ctx.sent_packets.add(save_packet);

    TRANSMIT_EVENT new_event;
    new_event.wr_id = ev.wr_id;
    new_event.psn = ctx.nPSN - 1;
    new_event.ack_req = true;
    events.add(new_event);

    ctx.LSN = ctx.LSN + 1;
}

void TransmitProcessor(TRANSMIT_EVENT ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {

    if(ctx.first_pkt_connection) {
        ALPHA_CHECK new_event;
        new_event.qp_id = ev.qp_id;
        int alpha_timeout = 55;
        ctx.timer_alpha.set_duration(microsecond(alpha_timeout));
        ctx.timer_alpha.start(new_event);

        DCQCN_TIMER new_event2;
        new_event2.qp_id = ev.qp_id;
        int DCQCN_timeout = 1500;
        ctx.timer_DCQCN_counter.set_duration(microsecond(DCQCN_timeout));
        ctx.timer_DCQCN_counter.start(new_event2);

        ctx.first_pkt_connection = false;
    }

    bool last_packet = true;
    BTHeader bth;
    UDPHeader udp;
    int number_packets_sent = 0;

    for(element_list in ctx.sent_packets) {
        pkt_t p = element_list.packet;
        // New note: extract_hdr 
        element_list.packet.extract_hdr(udp);
        element_list.packet.extract_hdr(bth);

        // only sends packets with psn greater than the one in ev (the previous were already sent)
        if(bth.psn >= ev.psn) {

            // if it is a SEND packet and LSN < SSN
            if(!(element_list.SSN <= ctx.LSN) && bth.opcode == 4 || bth.opcode == 0 || bth.opcode == 2 || bth.opcode == 1) {
                bth.ack_req = true;
                element_list.packet.add_hdr(bth);
                element_list.packet.add_hdr(udp);
                // New note: is flow_id an integer? If I want to store this value, can I assign it to an integer?
                ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);
                number_packets_sent = number_packets_sent + 1;
                break;
            }

            element_list.packet.add_hdr(bth);
            element_list.packet.add_hdr(udp);
            ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);
            number_packets_sent = number_packets_sent + 1;

            ctx.byte_counter = ctx.byte_counter + p.len();
            if(ctx.tx_feedback.able && ctx.byte_counter > ctx.BC) {
                // increase byte counter for DCQCN    
                ctx.byte_counter = 0;
                ctx.BC = ctx.BC + 1;

                DCQCN_INCREASE new_event;
                new_event.qp_id = ev.qp_id;
                events.add(new_event);
            }

            // if it requests ack, we add and start another timer to the list ack_timeout
            if(bth.ack_req) {
                TRANSMIT_EVENT new_event;
                new_event.wr_id = ev.wr_id;
                new_event.psn = bth.psn;
                new_event.ack_req = ev.ack_req;

                ack_wait ack;
                ack.ack_timer.set_duration(microsecond(ctx.transport_timer));
                ack.ack_timer.start(new_event);
                ack.psn = bth.psn;
                ctx.ack_timeout.add(ack);
            }

            // transmit queue cannot receive packets (full)
            if(!ctx.tx_feedback.able) {
                last_packet = false;
                break;
            }
        } else {
            element_list.packet.add_hdr(bth);
            element_list.packet.add_hdr(udp);
        }
    }

    if(!last_packet) {
        TRANSMIT_EVENT new_event;
        new_event.wr_id = ev.wr_id;
        new_event.psn = ev.psn + number_packets_sent;
        new_event.ack_req = ev.ack_req;
        events.add(new_event);
    }
    
    tx.transmit_queue[get_flow_id(ev)].rate(ctx.Rc);
}

void RecvDataProcessor(RECV_DATA ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    pkt_t p = new_pkt();

    if(ctx.credit_count > 0) {
        if(ev.psn == ctx.ePSN && ev.opcode == 0 || ev.opcode == 4) {        // saves the psn of send operation's first packet
            ctx.recv_first_psn = ev.psn;
        }
        AETHeader aeth;
        if(ev.psn <= ctx.ePSN) {        // duplicate or expected PSN
            aeth.ack = true;
            aeth.MSN = ctx.MSN;
            if(ev.psn == ctx.ePSN && ev.opcode == 2 || ev.opcode == 4)        // last or only packet
                ctx.credit_count = ctx.credit_count - 1;
            aeth.credit_count = ctx.credit_count;
        } else {                        // out-of-order: NACK
            aeth.ack = false;
            aeth.type_nack = true;
        }
        BTHeader bth;
        bth.opcode = 17;
        bth.dest_qp = ctx.dest_qp;
        bth.psn = ctx.ePSN;
        UDPHeader udp;
        udp.src_port = ctx.src_port;
        udp.dst_port = ctx.dst_port;
        udp.length = 8 + bth.len() + aeth.len();
        p.add_hdr({udp, bth, aeth});

        // if the arriving packet matches the expected psn, we write to memory
        if(ev.psn == ctx.ePSN) {
            ctx.ePSN = ctx.ePSN + 1;
            addr_t addr = ctx.RQ_list[0].addr;
            mem_write(addr, ev.data, ev.psn - ctx.recv_first_psn, ev.data.len());

            if(ev.opcode == 2 || ev.opcode == 4) {
                work_comp cqe;
                cqe.wr_id = ev.wr_id;
                cqe.status = 0;
                cqe.opcode = 6;
                cqe.qp_id = ev.qp_id;
                cqe.length = ev.data.len();
                ctx.CQ.add(byte(cqe));

                ctx.RQ_list.remove();
            }
        }
        if(ev.ack_req || !aeth.ack)     // sends packet if requires ack or is a nack
            ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);

    } else {                // there are no recv events posted
        AETHeader aeth;
        aeth.ack = false;
        aeth.type_nack = false;
        BTHeader bth;
        bth.opcode = 17;
        bth.dest_qp = ctx.dest_qp;
        bth.psn = ctx.ePSN;
        UDPHeader udp;
        udp.src_port = ctx.src_port;
        udp.dst_port = ctx.dst_port;
        p.add_hdr({udp, bth, aeth});
        ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);
    }
}

void WriteDataProcessor(WRITE_DATA ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    if(ev.psn == ctx.ePSN && ev.opcode == 6 || ev.opcode == 10) {        // saves the psn of write operation's first/only packet
        ctx.write_first_psn = ev.psn;
    }
    AETHeader aeth;
    if(ev.psn <= ctx.ePSN) {        // duplicate or expected PSN
        aeth.ack = true;
        aeth.MSN = ctx.MSN;
        aeth.credit_count = ctx.credit_count;
    } else {                        // out-of-order: NACK
        aeth.ack = false;
        aeth.type_nack = true;
    }
    BTHeader bth;
    bth.opcode = 17;
    bth.dest_qp = ctx.dest_qp;
    bth.psn = ctx.ePSN;
    UDPHeader udp;
    udp.src_port = ctx.src_port;
    udp.dst_port = ctx.dst_port;
    udp.length = 8 + bth.len() + aeth.len();
    p.add_hdr({udp, bth, aeth});

    if(ev.psn == ctx.ePSN) {
        ctx.ePSN = ctx.ePSN + 1;
        addr_t addr = ctx.RQ_list[0].addr;
        mem_write(addr, ev.data, ev.psn - ctx.write_first_psn, ev.data.len());
    }
    if(ev.ack_req || !aeth.ack)     // sends packet if requires ack or is a nack
        ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);
}

void ReadReqProcessor(READ_REQ_DATA ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    pkt_t p = new_pkt();

    AETHeader aeth;
    BTHeader bth;
    if(ev.psn == ctx.ePSN) {        // expected PSN
        stream buffer;
        buffer.mem_append(ev.raddr, ev.length);
        bool last_packet = false;

        for(int index = 0; index < ctx.length; index = index + ctx.MTU) {
            int data_len = ctx.MTU;
            if(index + ctx.MTU < ev.length)
                p.add_data(buffer[index : index + ctx.MTU]);
            else { // if the last packet doesn't match MTU
                p.add_data(buffer[index : ev.length]);
                data_len = ev.length - index;
            }
            aeth.ack = true;
            aeth.MSN = ctx.MSN;
            aeth.credit_count = ctx.credit_count;
            if(ev.length <= ctx.MTU) {
                bth.opcode = 16;     // only 1 packet
                last_packet = true;
            } else if(index == 0)
                bth.opcode = 13;     // first packet
            else if(index + ctx.MTU >= ev.length) {
                bth.opcode = 15;     // last packet
                last_packet = true;
            } else
                bth.opcode = 14;     // middle packet
            bth.dest_qp = ctx.dest_qp;
            bth.psn = ctx.ePSN;
            UDPHeader udp;
            udp.src_port = ctx.src_port;
            udp.dst_port = ctx.dst_port;
            if(bth.opcode != 14) {
                udp.length = 8 + bth.len() + aeth.len() + data_len;
                p.add_hdr({udp, bth, aeth});
            }
            else {
                udp.length = 8 + bth.len() + data_len;
                p.add_hdr({udp, bth});
            }

            ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);
            ctx.ePSN = ctx.ePSN + 1;
        }
        // Note: maybe we'll probably need a different TRANSMIT_EVENT processor, just for the responder side
        if(!last_packet) {
            TRANSMIT_EVENT new_event;
            new_event.wr_id = ev.wr_id;
            new_event.psn = first_psn;
            new_event.ack_req = true;
            events.add(new_event);
        }

    } else if(ev.psn < ctx.ePSN) {  // duplicate
        aeth.ack = true;
        aeth.MSN = ctx.MSN;
        aeth.credit_count = ctx.credit_count;
        bth.opcode = 17;
        bth.dest_qp = ctx.dest_qp;
        bth.psn = ctx.ePSN;
        UDPHeader udp;
        udp.src_port = ctx.src_port;
        udp.dst_port = ctx.dst_port;
        udp.length = 8 + bth.len() + aeth.len();
        p.add_hdr({udp, bth, aeth});
        ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);
    } else {                        // out-of-order: NACK
        aeth.ack = false;
        aeth.type_nack = true;
        bth.opcode = 17;
        bth.dest_qp = ctx.dest_qp;
        bth.psn = ctx.ePSN;
        UDPHeader udp;
        udp.src_port = ctx.src_port;
        udp.dst_port = ctx.dst_port;
        udp.length = 8 + bth.len() + aeth.len();
        p.add_hdr({udp, bth, aeth});
        ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);
    }
}

void ReadRespProcessor(READ_RESP_DATA ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {

    if(ev.psn >= ctx.sent_packets[0].psn) {
        int inactive_timer_counter = 0;
        bool stop_counter = false;

        for(element in ctx.ack_timeout) {
            // stops the ack_timer of the event with the given psn
            if(ev.psn == element.psn) {
                element.ack_timer.stop();
                break;
            }
            if(!element.ack_timer.isActive() && !stop_counter) {
                inactive_timer_counter = inactive_timer_counter + 1;
            } else if (element.ack_timer.isActive()) {
                stop_counter = true;
            }
        }
        // remove the inactive elements until the first active or until the one with the expected psn
        for(int i = 0; i < inactive_timer_counter; i = i + 1)
            ctx.ack_timeout.remove();

        int pkt_counter = 0;
        int psn_ack_req = ev.psn;

        for(packet in ctx.sent_packets) {
            if(packet.ack_req) {
                psn_ack_req = packet.psn;       // gets the psn of the oldest packet that requires ack
                break;
            } else if(packet.psn > ev.psn)
                break;
            pkt_counter = pkt_counter + 1;
        }

        if(psn_ack_req != ev.psn) {             // if there is a packet that requires ack before the acked packet
            TRANSMIT_EVENT new_event;
            new_event.psn = psn_ack_req;
            new_event.ack_req = ev.true;
            events.add(new_event);
        } else {
            BTHeader bth = ctx.packet_list[pkt_counter].packet.extract_hdr(bth);
            addr_t addr = ctx.packet_list[pkt_counter].addr;
            mem_write(addr, ev.data, ev.psn - bth.psn, ev.data.len());
        }

        for(int i = 0; i <= pkt_counter; i = i + 1)
            ctx.sent_packets.remove();
    }
}

void AtomicDataProcessor(ATOMIC_DATA ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    pkt_t p = new_pkt();

    AtomicAckETHeader atomicAeth;
    AETHeader aeth;
    BTHeader bth;
    if(ev.psn == ctx.ePSN) {        // expected PSN
        stream local_data;
        // New note: I noticed that ev shouldn't have a length value. Since local_data will be converted into an integer, could
        // we use something like 4 instead, to represent 4 bytes?
        local_data.mem_append(ev.raddr, ev.length);
        // Note: how should we transform a stream into an integer, to compare or add with ev.compare and ev.switch_add? (or vice-versa)
        // A: we could use int as it is
        int data = int(local_data);

        if(ev.compare == -1) {      // fetchAdd
            int result = data + ev.swap_add;
            stream result_data;
            result_data.append(byte(result));
            mem_write(ev.raddr, result_data, 0, result_data.len());
        } else {                    // compSwap
            if(ev.compare == data) {
                stream result_data;
                result_data.append(byte(ev.swap_add));
                mem_write(ev.raddr, result_data, 0, result_data.len());
            }
        }
        atomicAeth.original_data = data;

        aeth.ack = true;
        aeth.MSN = ctx.MSN;
        aeth.credit_count = ctx.credit_count;

        bth.opcode = 18;
        bth.dest_qp = ctx.dest_qp;
        bth.psn = ctx.ePSN;

        UDPHeader udp;
        udp.src_port = ctx.src_port;
        udp.dst_port = ctx.dst_port;

        udp.length = 8 + bth.len() + aeth.len() + atomicAeth.len();
        p.add_hdr({udp, bth, aeth, atomicAeth});
        ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);

        ctx.ePSN = ctx.ePSN + 1;

    } else if(ev.psn < ctx.ePSN) {  // duplicate
        aeth.ack = true;
        aeth.MSN = ctx.MSN;
        aeth.credit_count = ctx.credit_count;
        bth.opcode = 17;
        bth.dest_qp = ctx.dest_qp;
        bth.psn = ctx.ePSN;
        UDPHeader udp;
        udp.src_port = ctx.src_port;
        udp.dst_port = ctx.dst_port;
        udp.length = 8 + bth.len() + aeth.len();
        p.add_hdr({udp, bth, aeth});
        ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);
    } else {                        // out-of-order: NACK
        aeth.ack = false;
        aeth.type_nack = true;
        bth.opcode = 17;
        bth.dest_qp = ctx.dest_qp;
        bth.psn = ctx.ePSN;
        UDPHeader udp;
        udp.src_port = ctx.src_port;
        udp.dst_port = ctx.dst_port;
        udp.length = 8 + bth.len() + aeth.len();
        p.add_hdr({udp, bth, aeth});
        ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);
    }   
}

void AckProcessor(ACK ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    int inactive_timer_counter = 0;
    bool stop_counter = false;

    for(element in ctx.ack_timeout) {
        // stops the ack_timer of the event with the given psn
        if(ev.psn == element.psn) {
            element.ack_timer.stop();
            break;
        }
        if(!element.ack_timer.isActive() && !stop_counter) {
            inactive_timer_counter = inactive_timer_counter + 1;
        } else if (element.ack_timer.isActive()) {
            stop_counter = true;
        }
    }
    // remove the inactive elements until the first active or until the one with the expected psn
    for(int i = 0; i < inactive_timer_counter; i = i + 1)
        ctx.ack_timeout.remove();

    int pkt_counter = 0;
    int psn_ack_req = ev.psn;

    for(packet in ctx.sent_packets) {
        if(packet.ack_req) {
            psn_ack_req = packet.psn;       // gets the psn of the oldest packet that requires ack
            break;
        } else if(packet.psn > ev.psn)
            break;
        pkt_counter = pkt_counter + 1;
    }

    for(int i = 0; i <= pkt_counter; i = i + 1)
        ctx.sent_packets.remove();

    if(psn_ack_req != ev.psn) {             // if there is a packet that requires ack before the acked packet
        TRANSMIT_EVENT new_event;
        new_event.psn = psn_ack_req;
        new_event.ack_req = ev.true;
        events.add(new_event);
    }

    ctx.LSN = ev.credit_count + ev.MSN;         // update LSN
}

void NackProcessor(NACK ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    int inactive_timer_counter = 0;
    bool stop_counter = false;

    for(element in ctx.ack_timeout) {
        // stops the ack_timer of the event with the given psn
        if(ev.psn == element.psn) {
            element.ack_timer.stop();
            break;
        }
        if(!element.ack_timer.isActive() && !stop_counter) {
            inactive_timer_counter = inactive_timer_counter + 1;
        } else if (element.ack_timer.isActive()) {
            stop_counter = true;
        }
    }
    // remove the inactive elements until the first active or until the one with the expected psn
    for(int i = 0; i < inactive_timer_counter; i = i + 1)
        ctx.ack_timeout.remove();

    if(ev.type_nack) {
        int pkt_counter = 0;
        int psn_ack_req = ev.psn;
        int wr_id = ctx.sent_packets[0].wr_id;

        for(packet in ctx.sent_packets) {
            if(packet.ack_req) {
                psn_ack_req = packet.psn;       // gets the psn of the oldest packet that requires ack
                break;
            } else if(packet.psn > ev.psn)
                break;
            
            pkt_counter = pkt_counter + 1;
        }

        for(int i = 0; i <= pkt_counter; i = i + 1)
            ctx.sent_packets.remove();

        TRANSMIT_EVENT new_event;
        new_event.psn = psn_ack_req;
        new_event.ack_req = ev.true;
        events.add(new_event);
        
    } else {
        TRANSMIT_EVENT new_event;
        new_event.psn = ev.psn;
        new_event.ack_req = ev.true;
        events.add(new_event);
    }
}

// This EP removes the elements from ctx's Sending Queue and creates a completion queue event, adding it to completion queue
void RemoveSQProcessor <T>(T ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    int remove_counter = 0;

    for(event in ctx.SQ_list) {
        if(ctx.packet_list[0].wr_id == event.wr_id)
            break;
        remove_counter = remove_counter + 1;
    }

    for(int i = 0; i < remove_counter; i = i + 1) {
        work_comp cqe;
        cqe.wr_id = ctx.SQ_list[i].wr_id;
        cqe.status = 0;
        // Note: could we use type(ctx.SQ_list[i]) to get the type of the event and, then, decide the opcode?
        // A: yes
        cqe.opcode = type(ctx.SQ_list[i]);
        cqe.qp_id = ctx.SQ_list[i].qp_id;
        cqe.length = ctx.SQ_list[i].data.len();
        ctx.CQ.add(byte(cqe));
        ctx.SQ_list.remove();
    }
}

void CnpProcessor <T>(T ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    if(ev.ecn) {
        ctx.ecn_found_counter = ctx.ecn_found_counter + 1;
        pkt_t p = new_pkt();

        BTHeader bth;
        bth.opcode = 129;
        bth.ack_req = false;
        bth.dest_qp = ev.qp_id;
        bth.psn = 0;

        UDPHeader udp;
        udp.src_port = ctx.src_port;
        udp.dst_port = ctx.dst_port;
        udp.length = 8 + bth.len();

        p.add_hdr({udp, bth});
        ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);

        CNP_CHECK new_event;
        new_event.qp_id = ev.qp_id;
        events.add(new_event);

        if(!ctx.timer_cnp.isActive()) {
            ctx.timer_cnp.set_duration(microsecond(50));
            ctx.timer_cnp.start(new_event);
            ctx.ecn_found_counter = 0;
        }
    }
}

void CnpRepeatProcessor (CNP_CHECK ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    if(ctx.ecn_found_counter > 0) {
        pkt_t p = new_pkt();

        BTHeader bth;
        bth.opcode = 129;
        bth.ack_req = false;
        bth.dest_qp = ev.qp_id;
        bth.psn = 0;

        UDPHeader udp;
        udp.src_port = ctx.src_port;
        udp.dst_port = ctx.dst_port;
        udp.length = 8 + bth.len();

        p.add_hdr({udp, bth});
        ctx.tx_feedback = tx.transmit_queue[get_flow_id(ev)].push(p);

        CNP_CHECK new_event;
        new_event.qp_id = ev.qp_id;
        events.add(new_event);

        ctx.timer_cnp.restart();
    } else
        ctx.timer_cnp.stop();
}

void CnpRateProcessor(CNP ev, myContext ctx, list<event_t> events, interm_out ou, tx_module txt) {
    int Rt = ctx.Rt;
    int Rc = ctx.Rc;
    int alpha = ctx.alpha;
    float g = 1/16;
    float k = 0.000055;

    ctx.Rt = Rc;
    ctx.Rc = Rc * (1 - alpha / 2);
    ctx.alpha = (1 - g) * alpha + g;

    ctx.timer_alpha.restart();
}

void AlphaCheckProcessor(ALPHA_CHECK ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    int alpha = ctx.alpha;
    float g = 1/16;

    ctx.alpha = (1 - g) * alpha;

    ctx.timer_alpha.restart();
}

void DcqcnTimerProcessor(DCQCN_TIMER ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    ctx.T = ctx.T + 1;

    DCQCN_INCREASE new_event;
    new_event.qp_id = ev.qp_id;
    events.add(new_event);

    timer_DCQCN_counter.restart();
}

void DcqcnIncreaseProcessor(DCQCN_INCREASE ev, myContext ctx, list<event_t> events, interm_out out, tx_module tx) {
    int max;
    int min;

    if(ctx.T > ctx.BC) {
        max = ctx.T;
        min = ctx.BC;
    } else if(ctx.T < ctx.BC) {
        max = ctx.BC;
        min = ctx.T;
    } else {
        max = ctx.BC;
        min = max;
    }

    if(max < ctx.F) {
        // Fast recovery
        ctx.Rc = (ctx.Rt + Rc) / 2;
    } else if(min > ctx.F) {
        // Hyper increase
        ctx.Rt = ctx.Rai * (min - ctx.F + 1);
        ctx.Rc = (ctx.Rt + ctx.Rc) / 2;
    } else {
        // Additive increase
        ctx.Rt = ctx.Rt + ctx.Rai;
        ctx.Rc = (ctx.Rt + ctx.Rc) / 2;
    }
}

dispatch table {
    RDMA_EVENT::SEND_WQE -> {SendProcessor};
    RDMA_EVENT::WRITE_WQE -> {WriteProcessor};
    RDMA_EVENT::RECV_WQE -> {RecvProcessor};
    RDMA_EVENT::READ_WQE -> {ReadProcessor};
    RDMA_EVENT::ATOMIC_WQE -> {AtomicProcessor};
    TRANSMIT_EVENT -> {TransmitProcessor};
    RECV_DATA -> {CnpCheckProcessor, RecvDataProcessor};
    WRITE_DATA -> {CnpCheckProcessor, WriteDataProcessor};
    READ_REQ_DATA -> {CnpCheckProcessor, ReadReqProcessor};
    READ_RESP_DATA -> {ReadRespProcessor, RemoveSQProcessor};
    ATOMIC_DATA -> {CnpCheckProcessor, AtomicDataProcessor};
    ACK -> {AckProcessor, RemoveSQProcessor};
    NACK -> {NackProcessor, RemoveSQProcessor};
    CNP_CHECK -> {CnpRepeatProcessor};
    CNP -> {CnpRateProcessor};
    DCQCN_TIMER -> {DcqcnTimerProcessor};
    DCQCN_INCREASE -> {DcqcnIncreaseProcessor};
}